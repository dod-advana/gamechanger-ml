{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Evaluation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our evaluations, we will be using 3 quantitative measures: (1) Precision, (2) Recall, and (3) Mean Reciprocal Rank (MRR). Each metric is evaluated at a specific _k_ value where _k_ is the number of top documents we will be using to evaluate it. This provides us insight on how the model behaves or how each piece of the search algorithm improves the overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Precision\n",
    "\n",
    "<img src=\"./eval_folder/precision.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In information retrieval, precision measures how many of the returned documents are relevant.\n",
    "\n",
    "\\begin{equation*}\n",
    "precision = \\frac{\\#\\;of\\;returned\\;relevant\\;documents}{\\#\\;of\\;returned\\;documents}\n",
    "\\end{equation*}\n",
    "\n",
    "Another way of thinking about it is if you have a precision score of 0.2, you can say \"In my top _k_ documents, 20% of them are relevant.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Recall\n",
    "\n",
    "<img src=\"./eval_folder/recall.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall measures how many of the relevant documents was the model able to retrieve.\n",
    "\n",
    "\\begin{equation*}\n",
    "recall = \\frac{\\#\\;of\\;all\\;possible\\;relevant\\;documents}{\\#\\;of\\;returned\\;relevant\\;documents}\n",
    "\\end{equation*}\n",
    "\n",
    "Another way of thinking about it is if you have a recall score of 0.86, you can say \"In my top _k_ documents, 86% of all relevant documents are there.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting MRR\n",
    "\n",
    "<img src=\"./eval_folder/mrr.png\" alt=\"Drawing\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In information retrieval, MRR measures how likely it is for a document to be returned at the top rank. A high MRR score means that your model is likely to return a relevant document at the top of the list.\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "MRR = \\frac{1}{i} \\sum_{k=1}^i \\frac{1}{R_i}\n",
    "\\end{equation*}\n",
    "\n",
    "where _R_ is the highest rank of any relevant document per query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
