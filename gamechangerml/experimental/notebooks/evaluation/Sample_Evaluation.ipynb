{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Dataset\n",
    "\n",
    "To download the dataset, you must first first initialize the environment variables with the `setup_env.sh` under `gamechangerml` with the `DEV` argument. You can then run the `dl_data_cli.py` under `gamechangerml/src/search/evaluation/`. It will prompt you to the name of the dataset and where the dataset will be downloaded. For this example, we will be using the `msmarco_1k` dataset.\n",
    "\n",
    "![](./assets/dl_dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(fpath):\n",
    "    with open(fpath, \"r\") as fp:\n",
    "        data = json.load(fp)\n",
    "    return data\n",
    "\n",
    "def save_json(data, fpath):\n",
    "    with open(fpath, \"w\") as fp:\n",
    "        json.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_json(\"./msmarco_1k/collection.json\")\n",
    "queries = load_json(\"./msmarco_1k/queries.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modifying the Documents or Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you can modify how the inputs and documents based on your model architecture. This can be adding tokens or words at the end or processing the text to your need. For this example, we'll simply perform an ASCII cleanup on the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    clean_text = re.sub(r'\\W+', ' ', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Leptin from Greek Î ÎµÏ Ï Ï Ï leptos thin â the hormone of energy expenditureâ is a hormone predominantly made by adipose cells that helps to regulate energy balance by inhibiting hunger Leptin is opposed by the actions of the hormone ghrelin the hunger hormone '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(documents['1333116'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as you maintain the mapping of the text to its corresponding document or query id, then it should be okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_docs = {key:clean_text(value) for key, value in documents.items()}\n",
    "new_queries = {key:clean_text(value) for key, value in queries.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticsearch Setup\n",
    "\n",
    "For this example, we'll setup an Elasticsearch container for test search. To run it, pull the image `elasticsearch:7.10.1`. If you don't have it, it should automatically pull it. On a separate terminal, you can run the command below.\n",
    "\n",
    "`docker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" elasticsearch:7.10.1`\n",
    "\n",
    "Some terminal logs should start populating but let it run for around 30 seconds for warm up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'documents'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting the mapping for the Elasticsearch index\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"doc_id\": {\"type\": \"text\"},\n",
    "            \"body\": {\"type\": \"text\"}\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "# Delete the index if it already exists\n",
    "es.indices.delete(index = \"documents\", ignore = [400, 404])\n",
    "\n",
    "# Creating the index\n",
    "es.indices.create(index = \"documents\", ignore = 400, body = mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9d935dbbe65d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0meven\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mwaiting\u001b[0m \u001b[0;36m60\u001b[0m \u001b[0mseconds\u001b[0m \u001b[0mbut\u001b[0m \u001b[0mstopping\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcode\u001b[0m \u001b[0mthen\u001b[0m \u001b[0mrunning\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfollowing\u001b[0m \u001b[0mcode\u001b[0m \u001b[0mblocks\u001b[0m \u001b[0mwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \"\"\"\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "I added a break here to stop running all of the code. For some reason, Elasticsearch doesn't accept the documents \n",
    "even after waiting 60 seconds but stopping the code then running the following code blocks work.\n",
    "\"\"\"\n",
    "assert 1 == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, [])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elastic_documents = []\n",
    "\n",
    "for doc_id, document in new_docs.items():\n",
    "    es_doc = {\n",
    "        \"_index\": \"documents\",\n",
    "        \"doc_id\": doc_id,\n",
    "        \"body\": document\n",
    "    }\n",
    "    elastic_documents.append(es_doc)\n",
    "    \n",
    "bulk(es, elastic_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(text, n_return = 100):\n",
    "    search_body = {\n",
    "        \"query\": {\n",
    "            \"match\": {\n",
    "                \"body\": text\n",
    "            }\n",
    "        },\n",
    "        \"size\": n_return\n",
    "    }\n",
    "    \n",
    "    answers = es.search(index = \"documents\", body = search_body)\n",
    "    answers = answers[\"hits\"][\"hits\"]\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_index': 'documents',\n",
       "  '_type': '_doc',\n",
       "  '_id': 'CFaXd3YBLF4VyGNYgJJu',\n",
       "  '_score': 10.982681,\n",
       "  '_source': {'doc_id': '7187227',\n",
       "   'body': 'PCNT stands for 1 8 PCNT Pericentrin Medical 2 5 similar PCNT Panama Canal Net Tonnage Business Tanker Cargo shipping 3 1 PCNT Panama Canal Nett Tonnage 4 3 PCNT Public Carrier Networks Technology Technology Telecom Telecommunications 5 1 PCNT Paideia Commentaries on the New Testament '}},\n",
       " {'_index': 'documents',\n",
       "  '_type': '_doc',\n",
       "  '_id': 'LVaXd3YBLF4VyGNYgJNv',\n",
       "  '_score': 4.495153,\n",
       "  '_source': {'doc_id': '1304031',\n",
       "   'body': ' and called us according to His own purpose 2 Timothy 1 9 KJV T o know one s purpose is to know who they themselves are and what they are doing They also have the determination to accomplish what it is that they are purposed to do '}}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(new_queries[\"1048579\"], n_return = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Answer File\n",
    "\n",
    "The EvalTool takes 2 JSON files to compare. The first is the predictions of the model and the second if the ground truth. The ground truth file comes with the evaluation dataset downloaded earlier and is called `relations.json`. The answer should have a JSON format shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample format\n",
    "sample_answer = {\n",
    "    \"query_id_1\": {\n",
    "        \"document_id_1\": 1,\n",
    "        \"document_id_2\": 2,\n",
    "        \"document_id_3\": 3\n",
    "    },\n",
    "    \"query_id_2\": {\n",
    "        \"document_id_4\": 1,\n",
    "        \"document_id_5\": 2,\n",
    "        \"document_id_6\": 3\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The keys of the dictionary refers to the query. The dictionary mapped with that key is the ranked set of documents where the key is the document id and the value is the rank of that document assigned by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_answers = {}\n",
    "for query_id, query in new_queries.items():\n",
    "    answer = search(query, n_return = 25)\n",
    "    query_answers = {}\n",
    "    for idx, doc in enumerate(answer):\n",
    "        query_answers[doc[\"_source\"][\"doc_id\"]] = idx + 1\n",
    "    model_answers[query_id] = query_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_json(model_answers, \"./eval_folder/answers.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Evaluation\n",
    "\n",
    "You can run the `evaltool.py` under `gamechangerml/src/search/evaluation/` and plug the answers and ground truth JSON file. Example below:\n",
    "\n",
    "`python gamechangerml/src/search/evaluation/evaltool.py -p gamechangerml/experimental/notebooks/evaluation/eval_folder/answers.json -g gamechangerml/experimental/notebooks/evaluation/msmarco_1k/relations.json -m gamechangerml/experimental/notebooks/evaluation/eval_folder/`\n",
    "\n",
    "The script will then compare the prediction with the ground truth and generate a `metrics.json` which contains the score at varying values of `k`. It will also generate graphs of these metrics at different values of `k`. Interpretations of these metrics are shown in the [Explaining Evaluation](./Explaining_Evaluation.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
