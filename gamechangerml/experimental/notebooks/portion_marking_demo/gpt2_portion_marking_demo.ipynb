{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code used here is all open source and is built on NeMo/JARVIS BERT/GPT2 models.  \n",
    "\n",
    "  You can pull the Docker container directly from ngc.nvidia.com or follow the build instructions from the open-source repo on GitHub:  https://github.com/deepmipt/DeepPavlov \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  The approach I demonstrate here provides a baseline starting point for the DoD-wide overclassification problem.  Stuart shared some background; basically, people are busy/lazy/overwhelmed and tend to mark the communications with a higher classification than needed.  Stuart also shared that there are over 1000 SCG documents that express the intent of security classification across the department.  I wanted a method that could scale to cover these many different guides, without actually having to access the S.C.G.s to train the model.  \n",
    " \n",
    " The initial approach was to structure the problem was to structure the portion marking decision as a language model 'question and answer' task.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  The question and answer set up involve providing inputs (e.g., a S.C.G.) and new text (e.g., a statement).  The language model's job is to give then an answer to the question (and confidence), and it provides the rationale (in this case, why a portion of the text may is classified). [Note: once we get to FOUO we can test it out on real data!]\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I like this approach because it should scale and generalize well (even though the example is simple) without having to see the S.C.G.s.  I think the product should highlight the portion of text and explain why a particular classification rule applies (the same way Grammarly does it would be nice)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(signed) RWL: rick.w.lentz.civ@mail.mil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
