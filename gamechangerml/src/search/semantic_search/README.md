# gamechangerml/src/search/semantic_search

The semantic search model is currently being developed and is not in the dev or prod instances of GAMECHANGER yet.

## How It Works

1. [`QueryGenerator`](./query_generator/query_generator.py)
    - Use synthetic query generation to create training data for document paragraphs.

2. [`QueryClassifier`](./query_classifier/query_classifier.py)
    - Use a grammatical correctness classifier to determine whether or not a query generated by `QueryGenerator` should be used for training.

3. [`BiEncoderTrainer`](./bi_encoder/train/bi_encoder_trainer.py)
    - First, use [`BiEncoderTrainingData`](./bi_encoder/train/bi_encoder_training_data.py) to transform text into training data. It utilizes the QueryGenerator and QueryClassifier classes.

        <details >
        <summary>Example</summary>

            query_generator = QueryGenerator()
            query_classifier = QueryClassifier()
            data_manager = BiEncoderTrainingData(query_generator, query_classifier)

            # Generate positive examples of related queries and paragraphs.
            data_manager.create_positive_examples(...)

        </details>

    - Then, finetune a bi-encoder model on our data.
        - Models tuned for cosine-similarity will prefer the retrieval of shorter passages, while models for dot-product will prefer the retrieval of longer passages. Thus, we are currently using **dot-product** with **normalized embeddings**. The dot-product computation is also *faster* than the cosine-similarity computation ([reference](https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search)).
        - In addition, we are using an **asymmetric** model ([reference](https://www.sbert.net/examples/applications/semantic-search/README.html#symmetric-vs-asymmetric-semantic-search)).

4. [`BiEncoder`](./bi_encoder/bi_encoder.py)
    - Create embeddings for corpus documents and save them in a pickle file.
    - At query time, embed the query using the same method that was used to embed the corpus. Then, use a similarity function to compare the embeddings and retrieve the most relevant documents in the corpus.

5. [`CrossEncoder`](./cross_encoder/cross_encoder.py)
    - Re-rank and score results of the BiEncoder using a cross encoder.


## Resources
- [Semantic Search with SBERT](https://www.sbert.net/examples/applications/semantic-search/README.html#)
