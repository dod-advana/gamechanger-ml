import os
import json
import numpy as np
import pandas as pd

from math import log, sqrt

import argparse

from collections import Counter

import matplotlib.pyplot as plt

class AblationStudy(object):
    def __init__(self, 
                 model_a_answer_path,
                 model_b_answer_path,
                 ground_truth_path,
                 results_path = "./",
                 model_a_name = "Model A",
                 model_b_name = "Model B"):
        """
        Class that performs a simple ablation study on the outputs
        generated by two different models and compares their ranking
        and performance in more detail.

        The predictions are expected to come in this format:

        {
            "query_id_1": {
                "document_id_1": 1,
                "document_id_2": 2,
                "document_id_3": 3
            }
        }

        Args:
            model_a_answer_path (string): File path of JSON or dictionary
                                          containing the predictions of
                                          Model A
            model_b_answer_path (string): File path of JSON or dictionary
                                          containing the predictions of
                                          Model B
            ground_truth_path (string): File path of JSON or dictionary
                                        containing the ground truth
            results_path (string): Directory path where all of the results and
                                   graphs will be stored
            model_a_name (string): Name of Model A
            model_b_name (string): Name of Model B
        """

        self.answer_a = self._load_json(model_a_answer_path)
        self.answer_b = self._load_json(model_b_answer_path)
        self.ground_truth = self._load_json(ground_truth_path)
        
        self.results_path = results_path

        self.model_a_name = model_a_name
        self.model_b_name = model_b_name

        self.model_scores = self._compute_ranks()

    def _load_json(self, fpath):
        """
        Loads a JSON file and returns a dictionary

        Args:
            fpath (string): File path of JSON

        Returns:
            data (dict): Dictionary of data from JSON file
        """
        with open(fpath, "r") as fp:
            data = json.load(fp)
        return data

    def _compute_ranks(self):
        """
        Loads a JSON file and returns a dictionary

        Args:
            fpath (string): File path of JSON

        Returns:
            data (dict): Dictionary of data from JSON file
        """
        ranks = {}
        for key in self.ground_truth:
            a_rank = self._get_best_rank(self.answer_a[key], self.ground_truth[key])
            b_rank = self._get_best_rank(self.answer_b[key], self.ground_truth[key])
            ranks[key] = {
                self.model_a_name: a_rank,
                self.model_b_name: b_rank
            }
        return ranks

    def _get_best_rank(self, ranks, truth):
        """
        Retrieves the highest rank of any of the relevant documents
        returned by the model.

        Args:
            ranks (dict): Dictionary with document-rank pairs
            truth (list): List of documents that are considered
                          relevant to the query

        Returns:
            best_rank (int): Highest rank of found relevant document. If
                             no relevant documents were found, -1 is
                             returned
        """
        best_rank = None
        for answer_key in truth:
            if answer_key in ranks:
                if best_rank is None:
                    best_rank = ranks[answer_key]
                else:
                    best_rank = min(best_rank, ranks[answer_key])
        if best_rank is None:
            return -1
        else:
            return best_rank

    def bar_plot_diff_10(self, a_count, b_count):
        """
        Plot occurence of models outperforming the other and returning
        the rank of the higher of the higher model.

        Args:
            a_count (dict): Dictionary of rank-count pairs of when 
                            Model A outperformed Model B where the rank
                            corresponds to the rank of Model A
            b_count (dict): Dictionary of rank-count pairs of when 
                            Model B outperformed Model A where the rank
                            corresponds to the rank of Model B
        """
        plt.figure(figsize = (10, 6))

        x = list(range(1, 11))
        x_a = [i-0.15 for i in x]
        x_b = [i+0.15 for i in x]
        new_a_count = []
        new_b_count = []
        for i in range(1, 11):
            if i in a_count:
                new_a_count.append(a_count[i])
            else:
                new_a_count.append(0)
            
            if i in b_count:
                new_b_count.append(b_count[i])
            else:
                new_b_count.append(0)

        plt.bar(x_a, new_a_count, width = 0.3, label = self.model_a_name)
        plt.bar(x_b, new_b_count, width = 0.3, label = self.model_b_name)

        plt.title('Occurence of where each\nmodel outperforms the other')
        plt.xlabel("Model Rank")
        plt.ylabel("Occurence")
        plt.xticks(x)
        plt.legend()
        plt.tight_layout()
        plt.grid()
        plt.savefig(os.path.join(self.results_path, "diff_10.png"))
        plt.clf()

    def plot_net_difference(self, scores):
        """
        Plot occurence of models outperforming the other and returning
        the rank of the higher of the higher model.

        Args:
            scores (pd.DataFrame): DataFrame containing highest ranks of
                                   each model at every query
        """
        scores["Difference"] = scores[self.model_a_name] - scores[self.model_b_name]
        difference = scores["Difference"].tolist()
        diff_count = dict(Counter(difference))

        plt.figure(figsize = (12, 6))

        x = list(range(1, 101))
        net_scores = []
        for i in x:
            a_better = diff_count[-i] if -i in diff_count else 0
            b_better = diff_count[i] if i in diff_count else 0
            net_scores.append(a_better - b_better)
        
        net_max = max(net_scores)
        net_min = min(net_scores)
        abs_max = max(abs(net_max), abs(net_min))
        abs_max = int(abs_max * 1.1)
        tab_space = " "*20

        plt.plot(x, net_scores, color = 'black')
        plt.hlines(y = 0, xmin = -100, xmax = 100, color = "black")

        plt.fill_between(x, 0, net_scores, 
                         alpha = 0.7,
                         where=[i>0 for i in net_scores],
                         interpolate = True,
                         label = f"'{self.model_a_name}' better")

        plt.fill_between(x, 0, net_scores, 
                         alpha = 0.7,
                         where=[i<0 for i in net_scores],
                         interpolate = True,
                         label = f"'{self.model_b_name}' better")

        plt.title("Net Improvement Scores")
        plt.xlabel("Score Improvement")
        plt.ylabel("Net Improvement Occurence")
        
        plt.xlim(-2, 102)
        plt.ylim((-abs_max, abs_max))
        plt.xticks(x)

        yticks, _ = plt.yticks()
        ylabels = [int(abs(i)) for i in yticks]

        plt.xticks([1] + [i*5 for i in range(1,21)])
        plt.yticks(yticks, ylabels)

        plt.grid()
        plt.tight_layout()
        plt.legend()
        plt.savefig(os.path.join(self.results_path, "net_improvement.png"))
        plt.clf()

    def plot_rank_count(self, scores):
        """
        Plot number of times the model's highest returned relevant document
        rank at varying ranks.

        Args:
            scores (pd.DataFrame): DataFrame containing highest ranks of
                                   each model at every query
        """
        plt.figure(figsize = (10, 6))

        a_count = dict(Counter(scores[self.model_a_name].tolist()))
        b_count = dict(Counter(scores[self.model_b_name].tolist()))

        x = list(range(1, 11))
        x_a = [i-0.15 for i in x]
        x_b = [i+0.15 for i in x]
        new_a_count = []
        new_b_count = []
        for i in range(1, 11):
            if i in a_count:
                new_a_count.append(a_count[i])
            else:
                new_a_count.append(0)
            
            if i in b_count:
                new_b_count.append(b_count[i])
            else:
                new_b_count.append(0)

        plt.bar(x_a, new_a_count, width = 0.3, label = self.model_a_name)
        plt.bar(x_b, new_b_count, width = 0.3, label = self.model_b_name)

        for text_x, text_y in zip(x_a, new_a_count):
            plt.text(text_x - 0.15, text_y + 50, text_y, va = "center", size = 8)
        
        for text_x, text_y in zip(x_b, new_b_count):
            plt.text(text_x - 0.15, text_y + 50, text_y, va = "center", size = 8)

        plt.title('Number of Model Rank Scores')
        plt.xlabel("Model Rank")
        plt.ylabel("Occurence")
        plt.xticks(x)
        plt.legend()
        plt.tight_layout()
        plt.grid()
        plt.savefig(os.path.join(self.results_path, "model_rank_occurence.png"))
        plt.clf()

    def plot_model_distribution(self, scores):
        """
        Plot number of times the model's highest returned relevant document
        rank at varying ranks.

        Args:
            scores (pd.DataFrame): DataFrame containing highest ranks of
                                   each model at every query
        """
        plt.figure(figsize = (8, 8))
        a_rank = scores[self.model_a_name].tolist()
        b_rank = scores[self.model_b_name].tolist()

        pairs = [(i, j) for i, j in zip(a_rank, b_rank)]
        pair_count = dict(Counter(pairs))

        raw_array = np.zeros((100, 100))
        array = np.zeros((101, 101))

        for pair, count in pair_count.items():
            raw_array[pair[1]-1, pair[0]-1] = count
            array[pair[1], pair[0]] = 0 if count == 0 else log(count)**0.5

        plt.imshow(array, cmap = 'BuGn', interpolation = 'none', origin = 'lower')
        for i in range(1, 21):
            for j in range(1, 21):
                plt.text(i - 0.25, j - 0.25, int(raw_array[j-1, i-1]), va = "center")
        plt.plot([-1, 100], [-1, 100])

        ticks = [i for i in range(1, 21)]

        plt.xticks(ticks)
        plt.yticks(ticks)

        plt.xlim(0.5, 20.5)
        plt.ylim(0.5, 20.5)

        plt.xlabel(f"{self.model_a_name} rank")
        plt.ylabel(f"{self.model_b_name} rank")
        plt.tight_layout()
        plt.savefig(os.path.join(self.results_path, "model_diff_dist.png"))
        plt.clf()

    def generate_lost_results(self, improve):
        """
        Generate the number of occurences where one model doesn't return
        any relevant document but the other does. The count is generated

        Args:
            scores (pd.DataFrame): DataFrame containing highest ranks of
                                   each model at every query
        """
        plt.figure(figsize = (12, 8))
        a_improve = improve[improve[self.model_b_name] == -1][self.model_a_name].tolist()
        b_improve = improve[improve[self.model_a_name] == -1][self.model_b_name].tolist()

        plt.hist(a_improve, bins = 20, alpha = 0.7, label = self.model_a_name)
        plt.hist(b_improve, bins = 20, alpha = 0.7, label = self.model_b_name)

        plt.yscale("log")

        plt.grid()
        plt.legend()
        plt.tight_layout()
        plt.savefig(os.path.join(self.results_path, "clear_improvement.png"))

    def generate_report(self):
        """
        Generates all the graphs and plots and saves them under the results path
        """
        scores = pd.DataFrame(self.model_scores).T.reset_index()
        clear_improvement = scores[(scores[self.model_a_name] == -1) | (scores[self.model_b_name] == -1)].reset_index(drop = True).copy()
        orig_scores = scores.copy()
        scores = scores[(scores[self.model_a_name] != -1) & (scores[self.model_b_name] != -1)].reset_index(drop = True).copy()
        model_a_better = scores[scores[self.model_a_name] < scores[self.model_b_name]].reset_index(drop = True).copy()
        model_b_better = scores[scores[self.model_b_name] < scores[self.model_a_name]].reset_index(drop = True).copy()
        model_tie = scores[scores[self.model_a_name] == scores[self.model_b_name]].copy()

        a_better_count = dict(Counter(model_a_better[self.model_a_name].tolist()))
        b_better_count = dict(Counter(model_b_better[self.model_b_name].tolist()))

        self.plot_rank_count(orig_scores)
        self.plot_net_difference(scores)
        self.bar_plot_diff_10(a_better_count, b_better_count)
        self.plot_model_distribution(scores)
        self.generate_lost_results(clear_improvement)

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "-a",
        "--a_model_prediction",
        dest="a_model_prediction",
        required=True,
        type=str,
        help="File path to JSON file with predictions of ranked retrieval for Model A",
    )
    parser.add_argument(
        "-b",
        "--b_model_prediction",
        dest="b_model_prediction",
        required=True,
        type=str,
        help="File path to JSON file with predictions of ranked retrieval for Model B",
    )
    parser.add_argument(
        "-g",
        "--ground_truth",
        dest="ground_truth",
        required=True,
        type=str,
        help="File path to JSON file with ground truth of ranked retrieval",
    )
    parser.add_argument(
        "-s",
        "--save-path",
        dest="save_path",
        required=True,
        type=str,
        help="Path to store the plots and results of the Ablation Study",
    )
    parser.add_argument(
        "--model-a-name",
        dest="model_a_name",
        required=False,
        default = "Model A",
        type=str,
        help="Name of Model A",
    )
    parser.add_argument(
        "--model-b-name",
        dest="model_b_name",
        required=False,
        default = "Model B",
        type=str,
        help="Name of Model B",
    )
    args = parser.parse_args()

    ablation = AblationStudy(
        args.a_model_prediction,
        args.b_model_prediction,
        args.ground_truth,
        results_path = args.save_path,
        model_a_name = args.model_a_name,
        model_b_name = args.model_b_name,
    )
    ablation.generate_report()

