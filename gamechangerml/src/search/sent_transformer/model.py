from txtai.embeddings import Embeddings
import os
import numpy as np
import pandas as pd
from gamechangerml.api.utils.logger import logger
from gamechangerml.src.utilities.test_utils import *
from gamechangerml.src.text_handling.process import preprocess
from gamechangerml.configs import EmbedderConfig
from gamechangerml.src.ml import SimilarityRanker

# TODO at end: simplify imports

class SentenceSearcher(object):
    """
    Imports the text index generated by the SentenceEncoder and
    performs the search functionality. Initial set of documents
    are first retrieved through an Annoy index then reranked with
    the similarity model.

    Args:
        index_path (str): Path to index directory generated by the
            SentenceEncoder
        encoder_model (str): Model name supported by huggingface
            and txtai to generate the document embeddings
        sim_model (str): Model name supported by huggingface
            and txtai to calculate similarity between query and document
    """

    def __init__(
        self, sim_model_name, index_path, transformer_path, sim_model=None
    ):

        self.embedder = Embeddings()
        self.embedder.load(index_path)
        # replace this with looking up ES
        self.data = pd.read_csv(
            os.path.join(index_path, "data.csv"), dtype={"paragraph_id": str}
        )
        try:
            silver_eval_file = get_most_recent_eval(
                os.path.join(index_path, "evals_gc", "silver")
            )
            silver_eval = open_json(
                silver_eval_file,
                os.path.join(index_path, "evals_gc", "silver"),
            )
            self.auto_threshold = EmbedderConfig.THRESHOLD_MULTIPLIER * float(
                silver_eval["best_threshold"]
            )
            logger.info(
                f"Setting automatic cutoff score to {self.auto_threshold}"
            )
        except Exception as e:
            logger.error(
                f"Do not have best threshold available in eval data, defaulting to {EmbedderConfig.DEFAULT_THRESHOLD}"
            )
            self.auto_threshold = EmbedderConfig.DEFAULT_THRESHOLD

        if sim_model is None:
            sim_model = SimilarityRanker(
                os.path.join(transformer_path, sim_model_name)
            )

        self.similarity = sim_model

    def retrieve_topn(self, query, num_results=10):
        retrieved = self.embedder.search(query, limit=num_results)
        results = []
        for doc_id, score in retrieved:
            doc = {}
            text = self.data[self.data["paragraph_id"] == str(doc_id)].iloc[0][
                "text"
            ]
            doc["id"] = doc_id
            doc["text"] = text
            doc["text_length"] = len(text)
            doc["score"] = score
            results.append(doc)
        return results

    def search(
        self,
        query,
        num_results=10,
        process=False,
        externalSim=True,
        threshold="auto",
    ):
    # TODO: update docstring (note incorrect return)
        """
        Search the index and perform a similarity scoring reranker at
        the topn returned documents
        Args:
            query (str): Query text to search in documents
        Returns:
            rerank (list): List of tuples following a (score, paragraph_id,
                paragraph_text) format ranked based on similarity with query
        """
        if process:
            query = " ".join(preprocess(query))
        if threshold == "auto":
            cutoff_score = self.auto_threshold
        else:
            try:
                threshold = float(threshold)
                if (
                    threshold < 1 and threshold > 0
                ):  # if a threshold is manually set betweeen 0-1, use that
                    cutoff_score = threshold
                    logger.info(f"Manually setting threshold to {threshold}")
            except:
                cutoff_score = self.auto_threshold

        logger.info(f"Sentence searching for: {query}")
        if len(query) > 2:
            top_results = self.retrieve_topn(query, num_results)
            # choose to use an external similarity transformer
            if externalSim:
                scores = self.similarity.rank(query, top_results)
                return [
                    {
                        "score": score,
                        "id": top_results[idx]["id"],
                        "text": top_results[idx]["text"],
                    }
                    for idx, score in scores
                ]
            else:
                # adding normalize text length to score and sorting
                finalResults = []
                result_text = [len(x["text"]) for x in top_results]
                length_scores = np.interp(
                    result_text, (min(result_text), max(result_text)), (0, 0.2)
                )
                for idx, doc in enumerate(top_results):
                    doc["text_length"] = length_scores[idx]
                    doc["score"] = doc["score"]
                    if doc["score"] >= cutoff_score:
                        doc["passing_result"] = 1
                    else:
                        doc["passing_result"] = 0
                    finalResults.append(doc)
                finalResults = sorted(
                    finalResults, key=lambda i: i["score"], reverse=True
                )

                return finalResults
        else:
            return []
